{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7663f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318b4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN: TensorFlow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91d59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create directories ===\n",
    "os.makedirs(\"../models/improve\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2923b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load cleaned & log-transformed data ===\n",
    "df = pd.read_csv(\"../data/improve/crop_data_pivot_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1c067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define target variables ===\n",
    "targets = [\"Production\", \"Area harvested\", \"Yield\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4079925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define ANN training function ===\n",
    "def train_ann(X_train, y_train, X_test, y_test, model_path):\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(64, activation='relu')(input_layer)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer='adam', loss=MeanSquaredError())\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_split=0.2,\n",
    "              epochs=50,\n",
    "              batch_size=32,\n",
    "              callbacks=[es],\n",
    "              verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    model.save(f\"{model_path}.h5\")\n",
    "    return y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262b1831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Training models to predict: Production\n",
      "🔹 Training ANN...\n",
      "\u001b[1m3599/3599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 717us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Random Forest...\n",
      "🔹 Training Linear Regression...\n",
      "🔹 Training XGBoost...\n",
      "\n",
      "📌 Training models to predict: Area harvested\n",
      "🔹 Training ANN...\n",
      "\u001b[1m3599/3599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Random Forest...\n",
      "🔹 Training Linear Regression...\n",
      "🔹 Training XGBoost...\n",
      "\n",
      "📌 Training models to predict: Yield\n",
      "🔹 Training ANN...\n",
      "\u001b[1m3599/3599\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 592us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Training Random Forest...\n",
      "🔹 Training Linear Regression...\n",
      "🔹 Training XGBoost...\n",
      "\n",
      "✅ All models trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# === Training loop for each target ===\n",
    "results = []\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"\\n📌 Training models to predict: {target}\")\n",
    "\n",
    "    # Features = other targets + 'Year'\n",
    "    feature_cols = [col for col in targets if col != target] + [\"Year\"]\n",
    "\n",
    "    # Drop rows with missing target/features\n",
    "    data = df.dropna(subset=[target] + feature_cols).copy()\n",
    "    X = data[feature_cols]\n",
    "    y = data[target]\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # === Train ANN ===\n",
    "    print(\"🔹 Training ANN...\")\n",
    "    y_pred_ann, model_ann = train_ann(X_train, y_train, X_test, y_test, f\"../models/{target}_ANN\")\n",
    "    joblib.dump(scaler, f\"../models/{target}_ANN_scaler.pkl\", compress=3)\n",
    "\n",
    "    mse_ann = mean_squared_error(y_test, y_pred_ann)\n",
    "    r2_ann = r2_score(y_test, y_pred_ann)\n",
    "\n",
    "    results.append({\"Target\": target, \"Model\": \"ANN\", \"MSE\": mse_ann, \"R2\": r2_ann})\n",
    "\n",
    "    # === Train Random Forest ===\n",
    "    print(\"🔹 Training Random Forest...\")\n",
    "    rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    joblib.dump({\"model\": rf, \"scaler\": scaler}, f\"../models/{target}_RandomForest.pkl\")\n",
    "\n",
    "    results.append({\"Target\": target, \"Model\": \"RandomForest\", \"MSE\": mean_squared_error(y_test, y_pred_rf), \"R2\": r2_score(y_test, y_pred_rf)})\n",
    "\n",
    "    # === Train Linear Regression ===\n",
    "    print(\"🔹 Training Linear Regression...\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    joblib.dump({\"model\": lr, \"scaler\": scaler}, f\"../models/{target}_LinearRegression.pkl\")\n",
    "\n",
    "    results.append({\"Target\": target, \"Model\": \"LinearRegression\", \"MSE\": mean_squared_error(y_test, y_pred_lr), \"R2\": r2_score(y_test, y_pred_lr)})\n",
    "\n",
    "    # === Train XGBoost ===\n",
    "    print(\"🔹 Training XGBoost...\")\n",
    "    xg = XGBRegressor(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "    xg.fit(X_train, y_train)\n",
    "    y_pred_xg = xg.predict(X_test)\n",
    "    joblib.dump({\"model\": xg, \"scaler\": scaler}, f\"../models/{target}_XGBoost.pkl\")\n",
    "\n",
    "    results.append({\"Target\": target, \"Model\": \"XGBoost\", \"MSE\": mean_squared_error(y_test, y_pred_xg), \"R2\": r2_score(y_test, y_pred_xg)})\n",
    "\n",
    "print(\"\\n✅ All models trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa580bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Model Evaluation Results:\n",
      "            Target             Model       MSE        R2\n",
      "5   Area harvested      RandomForest  0.011598  0.998755\n",
      "7   Area harvested           XGBoost  0.020844  0.997762\n",
      "6   Area harvested  LinearRegression  0.042174  0.995473\n",
      "4   Area harvested               ANN  0.085209  0.990853\n",
      "1       Production      RandomForest  0.002192  0.999772\n",
      "3       Production           XGBoost  0.010073  0.998951\n",
      "2       Production  LinearRegression  0.041392  0.995688\n",
      "0       Production               ANN  0.067210  0.992999\n",
      "9            Yield      RandomForest  0.020535  0.989483\n",
      "8            Yield               ANN  0.028656  0.985324\n",
      "11           Yield           XGBoost  0.031472  0.983882\n",
      "10           Yield  LinearRegression  0.042631  0.978167\n"
     ]
    }
   ],
   "source": [
    "# === Summary table ===\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=[\"Target\", \"R2\"], ascending=[True, False])\n",
    "print(\"\\n📊 Model Evaluation Results:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
